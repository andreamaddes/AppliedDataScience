{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - RMSE: 1048377.8954173692, R²: -3.313421908741585\n",
      "Random Forest - RMSE: 328168.5317942891, R²: 0.5773507554563654\n",
      "Gradient Boosting - RMSE: 335950.2096867705, R²: 0.5570690152593278\n",
      "XGBoost - RMSE: 324795.202749959, R²: 0.5859951383625199\n",
      "Best model: XGBoost\n",
      "Best parameters for XGBoost: {'regressor__learning_rate': 0.2, 'regressor__max_depth': 7, 'regressor__n_estimators': 200}\n",
      "Final model - RMSE: 324148.9920587084, R²: 0.5876409029743577\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('NLP_enriched_immoscout24.csv')\n",
    "\n",
    "# Select relevant features and target\n",
    "features = df[['Title', 'Rooms', 'Living Space (sqm)', 'canton', 'Distance from nearest station (m)', 'city_center', 'garden', 'terrace', 'view', 'luxus', 'condition']]\n",
    "target = df['Price']\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_features = ['Rooms', 'Living Space (sqm)', 'Distance from nearest station (m)']\n",
    "categorical_features = ['Title', 'canton', 'city_center', 'garden', 'terrace', 'view', 'luxus', 'condition']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Create a pipeline that preprocesses the data and then applies a regressor\n",
    "def create_pipeline(regressor):\n",
    "    return Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('regressor', regressor)])\n",
    "\n",
    "# List of models to train\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    pipeline = create_pipeline(model)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results[name] = {'RMSE': rmse, 'R²': r2}\n",
    "    print(f'{name} - RMSE: {rmse}, R²: {r2}')\n",
    "\n",
    "# Find the best model\n",
    "best_model_name = min(results, key=lambda k: results[k]['RMSE'])\n",
    "best_model = models[best_model_name]\n",
    "print(f'Best model: {best_model_name}')\n",
    "\n",
    "# Optionally, perform hyperparameter tuning on the best model\n",
    "param_grid = {}\n",
    "if best_model_name == 'Random Forest':\n",
    "    param_grid = {\n",
    "        'regressor__n_estimators': [100, 200],\n",
    "        'regressor__max_depth': [10, 20, None]\n",
    "    }\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    param_grid = {\n",
    "        'regressor__n_estimators': [100, 200],\n",
    "        'regressor__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'regressor__max_depth': [3, 5, 7]\n",
    "    }\n",
    "elif best_model_name == 'XGBoost':\n",
    "    param_grid = {\n",
    "        'regressor__n_estimators': [100, 200],\n",
    "        'regressor__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'regressor__max_depth': [3, 5, 7]\n",
    "    }\n",
    "\n",
    "if param_grid:\n",
    "    grid_search = GridSearchCV(create_pipeline(best_model), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f'Best parameters for {best_model_name}: {grid_search.best_params_}')\n",
    "    best_pipeline = grid_search.best_estimator_\n",
    "else:\n",
    "    best_pipeline = create_pipeline(best_model)\n",
    "    best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Final evaluation on the test set\n",
    "final_y_pred = best_pipeline.predict(X_test)\n",
    "final_rmse = mean_squared_error(y_test, final_y_pred, squared=False)\n",
    "final_r2 = r2_score(y_test, final_y_pred)\n",
    "print(f'Final model - RMSE: {final_rmse}, R²: {final_r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 323783.9383847122\n",
      "R-squared: 0.5885691701755038\n",
      "canton_Geneva: 0.14813823997974396\n",
      "canton_Jura: 0.09577798843383789\n",
      "canton_Zurich: 0.09133481979370117\n",
      "Living Space (sqm): 0.06614197045564651\n",
      "canton_Zug: 0.053356271237134933\n",
      "canton_Neuchatel: 0.04513109102845192\n",
      "canton_Solothurn: 0.04482155293226242\n",
      "canton_Valais: 0.04110576584935188\n",
      "canton_Schwyz: 0.0392439141869545\n",
      "canton_Vaud: 0.03872501477599144\n",
      "canton_Basel-landschaft: 0.029934730380773544\n",
      "canton_Fribourg: 0.02849418856203556\n",
      "canton_Basel-stadt: 0.028417708352208138\n",
      "canton_Lucerne: 0.026595642790198326\n",
      "canton_Graubuenden: 0.025104302912950516\n",
      "canton_Bern: 0.023697303608059883\n",
      "luxus_False: 0.015972863882780075\n",
      "canton_Ticino: 0.014517085626721382\n",
      "canton_Nidwalden: 0.014120751991868019\n",
      "canton_St-gallen: 0.013703402131795883\n",
      "view_False: 0.01272311620414257\n",
      "canton_Schaffhausen: 0.01177024096250534\n",
      "canton_Glarus: 0.010760042816400528\n",
      "canton_Obwalden: 0.010484874248504639\n",
      "condition_renovated: 0.008455581963062286\n",
      "condition_new: 0.006724267266690731\n",
      "canton_Uri: 0.006709916517138481\n",
      "Rooms: 0.005857286974787712\n",
      "Distance from nearest station (m): 0.005800620187073946\n",
      "canton_Thurgau: 0.005641939118504524\n",
      "canton_Aargau: 0.005587500054389238\n",
      "terrace_False: 0.005514776799827814\n",
      "garden_False: 0.0049698930233716965\n",
      "city_center_False: 0.0046570925042033195\n",
      "condition_old: 0.003950636833906174\n",
      "canton_Appenzell-ausser-rhoden: 0.003617876907810569\n",
      "canton_Appenzell-inner-rhoden: 0.0024397927336394787\n",
      "city_center_True: 0.0\n",
      "garden_True: 0.0\n",
      "terrace_True: 0.0\n",
      "view_True: 0.0\n",
      "luxus_True: 0.0\n",
      "Best parameters: {'regressor__learning_rate': 0.1, 'regressor__max_depth': 5, 'regressor__n_estimators': 300}\n",
      "Best Model Root Mean Squared Error: 321757.7624016023\n",
      "Best Model R-squared: 0.5937023643683965\n",
      "canton_Geneva: 0.16094359755516052\n",
      "canton_Jura: 0.13286450505256653\n",
      "canton_Zurich: 0.08971188962459564\n",
      "canton_Zug: 0.06352001428604126\n",
      "Living Space (sqm): 0.04950748011469841\n",
      "canton_Neuchatel: 0.046771127730607986\n",
      "canton_Schwyz: 0.04380977526307106\n",
      "canton_Valais: 0.04308278113603592\n",
      "canton_Solothurn: 0.039122626185417175\n",
      "canton_Vaud: 0.029615482315421104\n",
      "canton_Basel-stadt: 0.027953939512372017\n",
      "canton_Fribourg: 0.026994554325938225\n",
      "canton_Graubuenden: 0.022424982860684395\n",
      "canton_Lucerne: 0.019677940756082535\n",
      "canton_Basel-landschaft: 0.017369894310832024\n",
      "canton_Bern: 0.015737934038043022\n",
      "luxus_False: 0.015094759874045849\n",
      "canton_Nidwalden: 0.015079312026500702\n",
      "canton_St-gallen: 0.011664088815450668\n",
      "canton_Schaffhausen: 0.011599617078900337\n",
      "canton_Ticino: 0.011097392998635769\n",
      "view_False: 0.010967069305479527\n",
      "canton_Glarus: 0.009965655393898487\n",
      "canton_Obwalden: 0.009697536937892437\n",
      "condition_renovated: 0.008421630598604679\n",
      "condition_new: 0.007096712477505207\n",
      "canton_Appenzell-ausser-rhoden: 0.006631558761000633\n",
      "Rooms: 0.006229075603187084\n",
      "canton_Uri: 0.006220598705112934\n",
      "terrace_False: 0.006219700910151005\n",
      "city_center_False: 0.0062192706391215324\n",
      "Distance from nearest station (m): 0.006091275252401829\n",
      "canton_Thurgau: 0.005934237036854029\n",
      "canton_Aargau: 0.0055375476367771626\n",
      "garden_False: 0.0052176713943481445\n",
      "condition_old: 0.003987832926213741\n",
      "canton_Appenzell-inner-rhoden: 0.0019189661834388971\n",
      "city_center_True: 0.0\n",
      "garden_True: 0.0\n",
      "terrace_True: 0.0\n",
      "view_True: 0.0\n",
      "luxus_True: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "file_path = 'NLP_enriched_immoscout24.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unwanted columns\n",
    "data = data.drop(['Address', 'Title', 'Description', 'Price_per_SquareMeter'], axis=1)\n",
    "\n",
    "# Select only numeric columns for median calculation\n",
    "numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].median())\n",
    "\n",
    "# Define categorical and numeric features\n",
    "categorical_features = ['canton', 'condition', 'city_center', 'garden', 'terrace', 'view', 'luxus']\n",
    "numeric_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numeric_features.remove('Price')  # Exclude the target variable\n",
    "\n",
    "# Preprocessing for numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the XGBoost regressor within a pipeline\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=300, learning_rate=0.05, max_depth=5)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', xgb_model)\n",
    "])\n",
    "\n",
    "# Train-test split\n",
    "X = data.drop('Price', axis=1)\n",
    "y = data['Price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model using root_mean_squared_error and R-squared\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f'R-squared: {r2}')\n",
    "\n",
    "# Feature importance (requires handling of feature names post OneHotEncoding)\n",
    "encoder_features = pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "features = numeric_features + list(encoder_features)\n",
    "importances = pipeline.named_steps['regressor'].feature_importances_\n",
    "\n",
    "# Print sorted feature importance\n",
    "feature_importance_dict = dict(zip(features, importances))\n",
    "sorted_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for name, importance in sorted_features:\n",
    "    print(f\"{name}: {importance}\")\n",
    "\n",
    "# Hyperparameter tuning for better performance\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 300, 500],\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict with the best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "rmse_best = np.sqrt(mean_squared_error(y_test, y_pred_best))\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "print(f'Best Model Root Mean Squared Error: {rmse_best}')\n",
    "print(f'Best Model R-squared: {r2_best}')\n",
    "\n",
    "# Feature importance of the best model\n",
    "best_importances = best_model.named_steps['regressor'].feature_importances_\n",
    "best_feature_importance_dict = dict(zip(features, best_importances))\n",
    "sorted_best_features = sorted(best_feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for name, importance in sorted_best_features:\n",
    "    print(f\"{name}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 320778.8291153795\n",
      "R-squared: 0.5924448548915662\n",
      "canton_Geneva: 0.14742432534694672\n",
      "canton_Jura: 0.1008109450340271\n",
      "canton_Zurich: 0.09320199489593506\n",
      "Living Space (sqm): 0.07012826949357986\n",
      "canton_Zug: 0.0586274228990078\n",
      "canton_Neuchatel: 0.049328550696372986\n",
      "canton_Solothurn: 0.044400714337825775\n",
      "canton_Valais: 0.04405668377876282\n",
      "canton_Vaud: 0.041255347430706024\n",
      "canton_Schwyz: 0.03397664800286293\n",
      "canton_Fribourg: 0.03007218800485134\n",
      "canton_Basel-stadt: 0.02975667268037796\n",
      "canton_Bern: 0.02420508675277233\n",
      "canton_Lucerne: 0.02143046259880066\n",
      "canton_Basel-landschaft: 0.021337559446692467\n",
      "canton_Graubuenden: 0.020872266963124275\n",
      "luxus_True: 0.016074851155281067\n",
      "canton_Nidwalden: 0.014581868425011635\n",
      "canton_Obwalden: 0.012249794788658619\n",
      "canton_Glarus: 0.011830893345177174\n",
      "condition_renovated: 0.01088027935475111\n",
      "canton_Schaffhausen: 0.010377221740782261\n",
      "canton_Ticino: 0.009930566884577274\n",
      "canton_St-gallen: 0.009791843593120575\n",
      "view_True: 0.00801895186305046\n",
      "condition_new: 0.007592515088617802\n",
      "canton_Uri: 0.007138012908399105\n",
      "canton_Aargau: 0.006935604382306337\n",
      "city_center_True: 0.006532345898449421\n",
      "canton_Appenzell-ausser-rhoden: 0.005952567793428898\n",
      "Distance from nearest station (m): 0.005550829228013754\n",
      "canton_Thurgau: 0.005505542270839214\n",
      "Rooms: 0.005466024857014418\n",
      "terrace_True: 0.005364485550671816\n",
      "garden_True: 0.004250495694577694\n",
      "condition_old: 0.002840328263118863\n",
      "canton_Appenzell-inner-rhoden: 0.002249745884910226\n",
      "Best parameters: {'regressor__learning_rate': 0.1, 'regressor__max_depth': 5, 'regressor__n_estimators': 300}\n",
      "Best Model Root Mean Squared Error: 316376.52239753545\n",
      "Best Model R-squared: 0.6035545110323166\n",
      "canton_Geneva: 0.1661403477191925\n",
      "canton_Jura: 0.12880419194698334\n",
      "canton_Zurich: 0.0937734916806221\n",
      "canton_Zug: 0.07037543505430222\n",
      "Living Space (sqm): 0.04940870404243469\n",
      "canton_Neuchatel: 0.04633572697639465\n",
      "canton_Schwyz: 0.03896883502602577\n",
      "canton_Valais: 0.03799300640821457\n",
      "canton_Solothurn: 0.03534211963415146\n",
      "canton_Basel-stadt: 0.03383002430200577\n",
      "canton_Vaud: 0.03061770275235176\n",
      "canton_Fribourg: 0.025289563462138176\n",
      "canton_Graubuenden: 0.01937798783183098\n",
      "canton_Bern: 0.019172463566064835\n",
      "canton_Lucerne: 0.019001541659235954\n",
      "canton_Basel-landschaft: 0.01871541142463684\n",
      "canton_Nidwalden: 0.016210105270147324\n",
      "luxus_True: 0.015036135911941528\n",
      "canton_Glarus: 0.01196786668151617\n",
      "canton_Obwalden: 0.011076265014708042\n",
      "canton_Schaffhausen: 0.00996976625174284\n",
      "condition_renovated: 0.009399959817528725\n",
      "canton_St-gallen: 0.009069934487342834\n",
      "canton_Ticino: 0.008023547939956188\n",
      "city_center_True: 0.00787973403930664\n",
      "view_True: 0.007743169087916613\n",
      "condition_new: 0.0076768482103943825\n",
      "canton_Uri: 0.007610280066728592\n",
      "terrace_True: 0.006633884739130735\n",
      "Distance from nearest station (m): 0.006306975148618221\n",
      "canton_Aargau: 0.0061766598373651505\n",
      "Rooms: 0.005704960320144892\n",
      "canton_Appenzell-ausser-rhoden: 0.005211581010371447\n",
      "garden_True: 0.004383713938295841\n",
      "condition_old: 0.004326744936406612\n",
      "canton_Thurgau: 0.004310918971896172\n",
      "canton_Appenzell-inner-rhoden: 0.0021343929693102837\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "file_path = 'NLP_enriched_immoscout24.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unwanted columns\n",
    "data = data.drop(['Address', 'Title', 'Description', 'Price_per_SquareMeter'], axis=1)\n",
    "\n",
    "# Filter out properties with prices under 200,000\n",
    "data = data[data['Price'] >= 200000]\n",
    "\n",
    "# Select only numeric columns for median calculation\n",
    "numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].median())\n",
    "\n",
    "# Define categorical and numeric features\n",
    "categorical_features = ['canton', 'condition', 'city_center', 'garden', 'terrace', 'view', 'luxus']\n",
    "numeric_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numeric_features.remove('Price')  # Exclude the target variable\n",
    "\n",
    "# Preprocessing for numeric and categorical data\n",
    "# Adjust OneHotEncoder to drop 'False' categories\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(drop='if_binary'), categorical_features)  # Drop the 'False' categories for binary features\n",
    "    ])\n",
    "\n",
    "# Define the XGBoost regressor within a pipeline\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=300, learning_rate=0.05, max_depth=5)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', xgb_model)\n",
    "])\n",
    "\n",
    "# Train-test split\n",
    "X = data.drop('Price', axis=1)\n",
    "y = data['Price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model using root_mean_squared_error and R-squared\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f'R-squared: {r2}')\n",
    "\n",
    "# Feature importance (requires handling of feature names post OneHotEncoding)\n",
    "encoder_features = pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "features = numeric_features + list(encoder_features)\n",
    "importances = pipeline.named_steps['regressor'].feature_importances_\n",
    "\n",
    "# Print sorted feature importance\n",
    "feature_importance_dict = dict(zip(features, importances))\n",
    "sorted_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for name, importance in sorted_features:\n",
    "    print(f\"{name}: {importance}\")\n",
    "\n",
    "# Hyperparameter tuning for better performance\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 300, 500],\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict with the best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "rmse_best = np.sqrt(mean_squared_error(y_test, y_pred_best))\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "print(f'Best Model Root Mean Squared Error: {rmse_best}')\n",
    "print(f'Best Model R-squared: {r2_best}')\n",
    "\n",
    "# Feature importance of the best model\n",
    "best_importances = best_model.named_steps['regressor'].feature_importances_\n",
    "best_feature_importance_dict = dict(zip(features, best_importances))\n",
    "sorted_best_features = sorted(best_feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for name, importance in sorted_best_features:\n",
    "    print(f\"{name}: {importance}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
