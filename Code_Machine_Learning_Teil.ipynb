{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - RMSE: 1048377.8954173692, R²: -3.313421908741585\n",
      "Random Forest - RMSE: 328168.5317942891, R²: 0.5773507554563654\n",
      "Gradient Boosting - RMSE: 335950.2096867705, R²: 0.5570690152593278\n",
      "XGBoost - RMSE: 324795.202749959, R²: 0.5859951383625199\n",
      "Best model: XGBoost\n",
      "Best parameters for XGBoost: {'regressor__learning_rate': 0.2, 'regressor__max_depth': 7, 'regressor__n_estimators': 200}\n",
      "Final model - RMSE: 324148.9920587084, R²: 0.5876409029743577\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('NLP_enriched_immoscout24.csv')\n",
    "\n",
    "# Select relevant features and target\n",
    "features = df[['Title', 'Rooms', 'Living Space (sqm)', 'canton', 'Distance from nearest station (m)', 'city_center', 'garden', 'terrace', 'view', 'luxus', 'condition']]\n",
    "target = df['Price']\n",
    "\n",
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_features = ['Rooms', 'Living Space (sqm)', 'Distance from nearest station (m)']\n",
    "categorical_features = ['Title', 'canton', 'city_center', 'garden', 'terrace', 'view', 'luxus', 'condition']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Create a pipeline that preprocesses the data and then applies a regressor\n",
    "def create_pipeline(regressor):\n",
    "    return Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('regressor', regressor)])\n",
    "\n",
    "# List of models to train\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    pipeline = create_pipeline(model)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results[name] = {'RMSE': rmse, 'R²': r2}\n",
    "    print(f'{name} - RMSE: {rmse}, R²: {r2}')\n",
    "\n",
    "# Find the best model\n",
    "best_model_name = min(results, key=lambda k: results[k]['RMSE'])\n",
    "best_model = models[best_model_name]\n",
    "print(f'Best model: {best_model_name}')\n",
    "\n",
    "# Optionally, perform hyperparameter tuning on the best model\n",
    "param_grid = {}\n",
    "if best_model_name == 'Random Forest':\n",
    "    param_grid = {\n",
    "        'regressor__n_estimators': [100, 200],\n",
    "        'regressor__max_depth': [10, 20, None]\n",
    "    }\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    param_grid = {\n",
    "        'regressor__n_estimators': [100, 200],\n",
    "        'regressor__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'regressor__max_depth': [3, 5, 7]\n",
    "    }\n",
    "elif best_model_name == 'XGBoost':\n",
    "    param_grid = {\n",
    "        'regressor__n_estimators': [100, 200],\n",
    "        'regressor__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'regressor__max_depth': [3, 5, 7]\n",
    "    }\n",
    "\n",
    "if param_grid:\n",
    "    grid_search = GridSearchCV(create_pipeline(best_model), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f'Best parameters for {best_model_name}: {grid_search.best_params_}')\n",
    "    best_pipeline = grid_search.best_estimator_\n",
    "else:\n",
    "    best_pipeline = create_pipeline(best_model)\n",
    "    best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Final evaluation on the test set\n",
    "final_y_pred = best_pipeline.predict(X_test)\n",
    "final_rmse = mean_squared_error(y_test, final_y_pred, squared=False)\n",
    "final_r2 = r2_score(y_test, final_y_pred)\n",
    "print(f'Final model - RMSE: {final_rmse}, R²: {final_r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 323783.9383847122\n",
      "canton_Geneva: 0.14813823997974396\n",
      "canton_Jura: 0.09577798843383789\n",
      "canton_Zurich: 0.09133481979370117\n",
      "Living Space (sqm): 0.06614197045564651\n",
      "canton_Zug: 0.053356271237134933\n",
      "canton_Neuchatel: 0.04513109102845192\n",
      "canton_Solothurn: 0.04482155293226242\n",
      "canton_Valais: 0.04110576584935188\n",
      "canton_Schwyz: 0.0392439141869545\n",
      "canton_Vaud: 0.03872501477599144\n",
      "canton_Basel-landschaft: 0.029934730380773544\n",
      "canton_Fribourg: 0.02849418856203556\n",
      "canton_Basel-stadt: 0.028417708352208138\n",
      "canton_Lucerne: 0.026595642790198326\n",
      "canton_Graubuenden: 0.025104302912950516\n",
      "canton_Bern: 0.023697303608059883\n",
      "luxus_False: 0.015972863882780075\n",
      "canton_Ticino: 0.014517085626721382\n",
      "canton_Nidwalden: 0.014120751991868019\n",
      "canton_St-gallen: 0.013703402131795883\n",
      "view_False: 0.01272311620414257\n",
      "canton_Schaffhausen: 0.01177024096250534\n",
      "canton_Glarus: 0.010760042816400528\n",
      "canton_Obwalden: 0.010484874248504639\n",
      "condition_renovated: 0.008455581963062286\n",
      "condition_new: 0.006724267266690731\n",
      "canton_Uri: 0.006709916517138481\n",
      "Rooms: 0.005857286974787712\n",
      "Distance from nearest station (m): 0.005800620187073946\n",
      "canton_Thurgau: 0.005641939118504524\n",
      "canton_Aargau: 0.005587500054389238\n",
      "terrace_False: 0.005514776799827814\n",
      "garden_False: 0.0049698930233716965\n",
      "city_center_False: 0.0046570925042033195\n",
      "condition_old: 0.003950636833906174\n",
      "canton_Appenzell-ausser-rhoden: 0.003617876907810569\n",
      "canton_Appenzell-inner-rhoden: 0.0024397927336394787\n",
      "city_center_True: 0.0\n",
      "garden_True: 0.0\n",
      "terrace_True: 0.0\n",
      "view_True: 0.0\n",
      "luxus_True: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea\\anaconda3\\envs\\adsenv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('NLP_enriched_immoscout24.csv')\n",
    "\n",
    "# Drop unwanted columns\n",
    "data = data.drop(['Address', 'Title', 'Description', 'Price_per_SquareMeter'], axis=1)\n",
    "\n",
    "# Select only numeric columns for median calculation\n",
    "numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].median())\n",
    "\n",
    "# Define categorical and numeric features\n",
    "categorical_features = ['canton', 'condition', 'city_center', 'garden', 'terrace', 'view', 'luxus']\n",
    "numeric_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numeric_features.remove('Price')  # Exclude the target variable\n",
    "\n",
    "# Preprocessing for numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the XGBoost regressor within a pipeline\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=300, learning_rate=0.05, max_depth=5)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', xgb_model)\n",
    "])\n",
    "\n",
    "# Train-test split\n",
    "X = data.drop('Price', axis=1)\n",
    "y = data['Price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model using RMSE\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "\n",
    "# Feature importance (requires handling of feature names post OneHotEncoding)\n",
    "encoder_features = pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "features = numeric_features + list(encoder_features)\n",
    "importances = pipeline.named_steps['regressor'].feature_importances_\n",
    "\n",
    "# Print sorted feature importance\n",
    "feature_importance_dict = dict(zip(features, importances))\n",
    "sorted_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for name, importance in sorted_features:\n",
    "    print(f\"{name}: {importance}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
