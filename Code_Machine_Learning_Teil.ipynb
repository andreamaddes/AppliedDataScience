{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 270494.5041411638\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('new_merged_cleaned_immoscout24.csv', delimiter=';')\n",
    "\n",
    "# Separate the target (Price) and features\n",
    "X = data.drop('Price', axis=1)\n",
    "y = data['Price']\n",
    "\n",
    "# Identify categorical and numeric features\n",
    "categorical_features = ['Canton']\n",
    "numeric_features = ['Rooms', 'Living Space (sqm)']\n",
    "\n",
    "# Preprocessor to handle categorical and numeric data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.001, random_state=42)\n",
    "\n",
    "# Create a pipeline with preprocessing and a random forest regressor\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "error = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean Absolute Error: {error}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a Mean Absolute Error (MAE) of 440,356.68 Swiss Francs in the context of predicting real estate prices, the interpretation of the results requires careful consideration due to the substantial size of the error relative to the expected values of property prices, even in a high-value market like Switzerland. This high MAE suggests significant discrepancies between the predicted values and the actual market prices. Hereâ€™s a detailed analysis suitable for a master's level understanding:\n",
    "\n",
    "1. Assessment of MAE\n",
    "The MAE being approximately 440,356 CHF is substantial, indicating that, on average, the predictions deviate from the actual sale prices by nearly half a million Swiss Francs. This level of error could be problematic for practical applications such as investment planning, loan issuance, and real estate valuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV] END regressor__max_depth=None, regressor__min_samples_leaf=1, regressor__n_estimators=100; total time=   5.3s\n",
      "[CV] END regressor__max_depth=None, regressor__min_samples_leaf=1, regressor__n_estimators=100; total time=   5.3s\n",
      "[CV] END regressor__max_depth=None, regressor__min_samples_leaf=1, regressor__n_estimators=100; total time=   5.0s\n",
      "[CV] END regressor__max_depth=None, regressor__min_samples_leaf=1, regressor__n_estimators=200; total time=  10.4s\n",
      "[CV] END regressor__max_depth=None, regressor__min_samples_leaf=1, regressor__n_estimators=200; total time=  10.5s\n",
      "[CV] END regressor__max_depth=None, regressor__min_samples_leaf=1, regressor__n_estimators=200; total time=  10.9s\n",
      "[CV] END regressor__max_depth=None, regressor__min_samples_leaf=2, regressor__n_estimators=100; total time=   4.3s\n",
      "[CV] END regressor__max_depth=None, regressor__min_samples_leaf=2, regressor__n_estimators=100; total time=   4.2s\n",
      "[CV] END regressor__max_depth=None, regressor__min_samples_leaf=2, regressor__n_estimators=100; total time=   4.3s\n",
      "[CV] END regressor__max_depth=None, regressor__min_samples_leaf=2, regressor__n_estimators=200; total time=   8.8s\n",
      "[CV] END regressor__max_depth=None, regressor__min_samples_leaf=2, regressor__n_estimators=200; total time=   8.7s\n",
      "[CV] END regressor__max_depth=None, regressor__min_samples_leaf=2, regressor__n_estimators=200; total time=   9.9s\n",
      "[CV] END regressor__max_depth=None, regressor__min_samples_leaf=4, regressor__n_estimators=100; total time=   4.0s\n",
      "[CV] END regressor__max_depth=None, regressor__min_samples_leaf=4, regressor__n_estimators=100; total time=   4.5s\n",
      "[CV] END regressor__max_depth=None, regressor__min_samples_leaf=4, regressor__n_estimators=100; total time=   4.3s\n",
      "[CV] END regressor__max_depth=None, regressor__min_samples_leaf=4, regressor__n_estimators=200; total time=   8.6s\n",
      "[CV] END regressor__max_depth=None, regressor__min_samples_leaf=4, regressor__n_estimators=200; total time=   8.1s\n",
      "[CV] END regressor__max_depth=None, regressor__min_samples_leaf=4, regressor__n_estimators=200; total time=   8.0s\n",
      "[CV] END regressor__max_depth=10, regressor__min_samples_leaf=1, regressor__n_estimators=100; total time=   1.9s\n",
      "[CV] END regressor__max_depth=10, regressor__min_samples_leaf=1, regressor__n_estimators=100; total time=   1.8s\n",
      "[CV] END regressor__max_depth=10, regressor__min_samples_leaf=1, regressor__n_estimators=100; total time=   1.8s\n",
      "[CV] END regressor__max_depth=10, regressor__min_samples_leaf=1, regressor__n_estimators=200; total time=   3.8s\n",
      "[CV] END regressor__max_depth=10, regressor__min_samples_leaf=1, regressor__n_estimators=200; total time=   3.5s\n",
      "[CV] END regressor__max_depth=10, regressor__min_samples_leaf=1, regressor__n_estimators=200; total time=   3.6s\n",
      "[CV] END regressor__max_depth=10, regressor__min_samples_leaf=2, regressor__n_estimators=100; total time=   1.7s\n",
      "[CV] END regressor__max_depth=10, regressor__min_samples_leaf=2, regressor__n_estimators=100; total time=   1.8s\n",
      "[CV] END regressor__max_depth=10, regressor__min_samples_leaf=2, regressor__n_estimators=100; total time=   1.7s\n",
      "[CV] END regressor__max_depth=10, regressor__min_samples_leaf=2, regressor__n_estimators=200; total time=   3.3s\n",
      "[CV] END regressor__max_depth=10, regressor__min_samples_leaf=2, regressor__n_estimators=200; total time=   3.3s\n",
      "[CV] END regressor__max_depth=10, regressor__min_samples_leaf=2, regressor__n_estimators=200; total time=   3.5s\n",
      "[CV] END regressor__max_depth=10, regressor__min_samples_leaf=4, regressor__n_estimators=100; total time=   1.5s\n",
      "[CV] END regressor__max_depth=10, regressor__min_samples_leaf=4, regressor__n_estimators=100; total time=   1.6s\n",
      "[CV] END regressor__max_depth=10, regressor__min_samples_leaf=4, regressor__n_estimators=100; total time=   1.6s\n",
      "[CV] END regressor__max_depth=10, regressor__min_samples_leaf=4, regressor__n_estimators=200; total time=   3.2s\n",
      "[CV] END regressor__max_depth=10, regressor__min_samples_leaf=4, regressor__n_estimators=200; total time=   3.3s\n",
      "[CV] END regressor__max_depth=10, regressor__min_samples_leaf=4, regressor__n_estimators=200; total time=   3.3s\n",
      "[CV] END regressor__max_depth=20, regressor__min_samples_leaf=1, regressor__n_estimators=100; total time=   4.1s\n",
      "[CV] END regressor__max_depth=20, regressor__min_samples_leaf=1, regressor__n_estimators=100; total time=   3.8s\n",
      "[CV] END regressor__max_depth=20, regressor__min_samples_leaf=1, regressor__n_estimators=100; total time=   3.9s\n",
      "[CV] END regressor__max_depth=20, regressor__min_samples_leaf=1, regressor__n_estimators=200; total time=   7.8s\n",
      "[CV] END regressor__max_depth=20, regressor__min_samples_leaf=1, regressor__n_estimators=200; total time=   7.6s\n",
      "[CV] END regressor__max_depth=20, regressor__min_samples_leaf=1, regressor__n_estimators=200; total time=   7.8s\n",
      "[CV] END regressor__max_depth=20, regressor__min_samples_leaf=2, regressor__n_estimators=100; total time=   3.4s\n",
      "[CV] END regressor__max_depth=20, regressor__min_samples_leaf=2, regressor__n_estimators=100; total time=   3.4s\n",
      "[CV] END regressor__max_depth=20, regressor__min_samples_leaf=2, regressor__n_estimators=100; total time=   3.5s\n",
      "[CV] END regressor__max_depth=20, regressor__min_samples_leaf=2, regressor__n_estimators=200; total time=   7.0s\n",
      "[CV] END regressor__max_depth=20, regressor__min_samples_leaf=2, regressor__n_estimators=200; total time=   7.0s\n",
      "[CV] END regressor__max_depth=20, regressor__min_samples_leaf=2, regressor__n_estimators=200; total time=   7.1s\n",
      "[CV] END regressor__max_depth=20, regressor__min_samples_leaf=4, regressor__n_estimators=100; total time=   3.1s\n",
      "[CV] END regressor__max_depth=20, regressor__min_samples_leaf=4, regressor__n_estimators=100; total time=   3.1s\n",
      "[CV] END regressor__max_depth=20, regressor__min_samples_leaf=4, regressor__n_estimators=100; total time=   3.2s\n",
      "[CV] END regressor__max_depth=20, regressor__min_samples_leaf=4, regressor__n_estimators=200; total time=   6.3s\n",
      "[CV] END regressor__max_depth=20, regressor__min_samples_leaf=4, regressor__n_estimators=200; total time=   6.3s\n",
      "[CV] END regressor__max_depth=20, regressor__min_samples_leaf=4, regressor__n_estimators=200; total time=   6.8s\n",
      "Best Model RMSE: 772264.9664723505\n",
      "Best Model R^2 Score: 0.5736841898932457\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Assuming 'data' is already loaded and preprocessor setup is available\n",
    "# Example setup for preprocessor (modify according to your actual feature categories)\n",
    "categorical_features = ['Canton']  # example categorical features\n",
    "numeric_features = ['Rooms', 'Living Space (sqm)']  # example numeric features\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Sample data preparation (ensure you replace this with your actual data)\n",
    "X = data.drop('Price', axis=1)\n",
    "y = data['Price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the pipeline with RandomForestRegressor\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Define a grid of parameters to search\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200],\n",
    "    'regressor__max_depth': [None, 10, 20],\n",
    "    'regressor__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(model_pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=2)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Best Model RMSE: {rmse}')\n",
    "print(f'Best Model R^2 Score: {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of the Results\n",
    "Root Mean Squared Error (RMSE) of 674,346: This still indicates a sizable average error in predictions, but it's much reduced from the previous RMSE values. It's important to contextualize this value within the range and scale of house prices in your dataset. If house prices typically range in the millions, this RMSE might be more acceptable.\n",
    "R^2 Score of 0.691: This score has improved markedly from earlier models, suggesting that the model is capturing a significant portion of the variability in the house prices. An R^2 Score closer to 1.0 is ideal, but 0.691 is a robust score, particularly for real-world data which can be noisy and unpredictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBRegressor\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Define the pipeline with XGBRegressor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m xgb_pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m      5\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),\n\u001b[0;32m      6\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb_regressor\u001b[39m\u001b[38;5;124m'\u001b[39m, XGBRegressor(objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg:squarederror\u001b[39m\u001b[38;5;124m'\u001b[39m, n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[0;32m      7\u001b[0m ])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define the pipeline with XGBRegressor\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xgb_regressor', XGBRegressor(objective='reg:squarederror', n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_xgb = xgb_pipeline.predict(X_test)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f'XGB Model RMSE: {rmse_xgb}')\n",
    "print(f'XGB Model R^2 Score: {r2_xgb}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
