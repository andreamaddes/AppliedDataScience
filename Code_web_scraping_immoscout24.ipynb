{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n",
      "Scraping page 26...\n",
      "Scraping page 27...\n",
      "Scraping page 28...\n",
      "Scraping page 29...\n",
      "Scraping page 30...\n",
      "Scraping page 31...\n",
      "Scraping page 32...\n",
      "Scraping page 33...\n",
      "Scraping page 34...\n",
      "Scraping page 35...\n",
      "Scraping page 36...\n",
      "Scraping page 37...\n",
      "Scraping page 38...\n",
      "Scraping page 39...\n",
      "Scraping page 40...\n",
      "Scraping page 41...\n",
      "Scraping page 42...\n",
      "Scraping page 43...\n",
      "Scraping page 44...\n",
      "Scraping page 45...\n",
      "Scraping page 46...\n",
      "Scraping page 47...\n",
      "Scraping page 48...\n",
      "Scraping page 49...\n",
      "Scraping page 50...\n",
      "                                    Address  \\\n",
      "0                               1963 Vétroz   \n",
      "1                       Dorf 7, 3305 Iffwil   \n",
      "2    Chemin de la Farette 4, 1232 Confignon   \n",
      "3           Lutisbachweg 10, 6315 Oberägeri   \n",
      "4       Allmendgütlistrasse 43, 8810 Horgen   \n",
      "..                                      ...   \n",
      "995                          1066 Epalinges   \n",
      "996     Impasse du Tronchet, 1740 Neyruz FR   \n",
      "997           Bernstrasse 33, 4852 Rothrist   \n",
      "998           Route de Lausanne, 1180 Rolle   \n",
      "999              6541 Sta. Maria in Calanca   \n",
      "\n",
      "                                                 Title  \\\n",
      "0    Vétroz - Appartement de 1.5 pièces avec grand ...   \n",
      "1    Moderne 4 1/2 Maisonette-Wohnung im historisch...   \n",
      "2             Appartement neuf avec jardin et terrasse   \n",
      "3                EDENBLICK - In 15 Minuten im Paradies   \n",
      "4               AURA - Letzte idyllische Gartenwohnung   \n",
      "..                                                 ...   \n",
      "995   Appartements de standing - Les Hauts d'Epalinges   \n",
      "996  Magnifiques 1.5 en plain-pied avec terrasse et...   \n",
      "997  2.5 Zimmer Terrassenwohnung (170 m2 WF) mit Li...   \n",
      "998    Un joyau architectural dans un cadre majestueux   \n",
      "999  Modernes Chalet \"Mezzo\" - hell und mit offener...   \n",
      "\n",
      "                                           Description      Rooms  \\\n",
      "0    EXCLUSIVITÉ SCHMIDTCet appartement de 1.5 pièc...  1.5 rooms   \n",
      "1    Das Areal Kreuz Iffwil wurde von 2022-2024 ren...  1.5 rooms   \n",
      "2    DESCRIPTION:Sur le coteau sud de Confignon ave...     1 room   \n",
      "3    Hier sind Sie auf der Sonnenseite zuhauseIn Ob...  1.5 rooms   \n",
      "4    Termin online buchen - bitte Link kopieren: ht...  1.5 rooms   \n",
      "..                                                 ...        ...   \n",
      "995  Cette nouvelle promotion est constituée de 8 a...     1 room   \n",
      "996                                                     1.5 rooms   \n",
      "997                                                     2.5 rooms   \n",
      "998  Embarquer à bord de la Baie d'Opale c'est vivr...  1.5 rooms   \n",
      "999                                                     2.5 rooms   \n",
      "\n",
      "    Living Space (sqm) Price  \n",
      "0                130m²     ,  \n",
      "1                120m²     ,  \n",
      "2                150m²     ,  \n",
      "3                140m²     ,  \n",
      "4                120m²     ,  \n",
      "..                 ...   ...  \n",
      "995               14m²     ,  \n",
      "996              140m²     ,  \n",
      "997              170m²     ,  \n",
      "998              150m²     ,  \n",
      "999              200m²     ,  \n",
      "\n",
      "[1000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_apartments(url):\n",
    "    # Setup ChromeDriver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run Chrome in headless mode (no GUI)\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    addresses = []\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    rooms = []\n",
    "    living_space = []\n",
    "    prices = []\n",
    "\n",
    "    # Setup ChromeDriver\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    # Loop through the 50 pages\n",
    "    for page_num in range(1, 51):  # Adjusted the range to include pages from 1 to 50\n",
    "        print(f\"Scraping page {page_num}...\")\n",
    "        page_url = f\"{url}?pn={page_num}\"  # Adjusted the URL structure\n",
    "        driver.get(page_url)\n",
    "        time.sleep(3)  # Add a delay to allow the page to load\n",
    "\n",
    "        # Get the full content of the website\n",
    "        source = driver.page_source\n",
    "\n",
    "        # Parse HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "        # Get addresses\n",
    "        address_elements = soup.find_all(class_='HgListingCard_address_JGiFv')\n",
    "        for element in address_elements:\n",
    "            address = element.find('address').text.strip()\n",
    "            addresses.append(address)\n",
    "\n",
    "        # Get titles and descriptions\n",
    "        title_desc_elements = soup.find_all(class_='HgListingDescription_description_r5HCO')\n",
    "        for element in title_desc_elements:\n",
    "            title = element.find('span').text.strip()\n",
    "            description_elem = element.find('p', class_='HgListingDescription_large_uKs3J')\n",
    "            description = description_elem.text.strip() if description_elem else ''\n",
    "            titles.append(title)\n",
    "            descriptions.append(description)\n",
    "\n",
    "        # Get rooms, living space, and prices\n",
    "        room_space_price_elements = soup.find_all(class_='HgListingRoomsLivingSpacePrice_roomsLivingSpacePrice_M6Ktp')\n",
    "        for element in room_space_price_elements:\n",
    "            strong_tags = element.find_all('strong')\n",
    "            rooms.append(strong_tags[0].text.strip()) if strong_tags else rooms.append('')\n",
    "            living_space.append(strong_tags[1].text.strip()) if len(strong_tags) > 1 else living_space.append('')\n",
    "            prices.append(element.find('span').text.strip())\n",
    "\n",
    "    # Close driver\n",
    "    driver.quit()\n",
    "\n",
    "    # Ensure all arrays have the same length\n",
    "    data_length = min(len(addresses), len(titles), len(descriptions), len(rooms), len(living_space), len(prices))\n",
    "\n",
    "    # Dataframe\n",
    "    df = pd.DataFrame({'Address': addresses[:data_length],\n",
    "                       'Title': titles[:data_length],\n",
    "                       'Description': descriptions[:data_length],\n",
    "                       'Rooms': rooms[:data_length],\n",
    "                       'Living Space (sqm)': living_space[:data_length],\n",
    "                       'Price': prices[:data_length]})\n",
    "\n",
    "    # Save to file\n",
    "    df.to_csv('immoscout24.csv', sep=\";\", index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Specify the URL to scrape\n",
    "url = 'https://www.immoscout24.ch/en/real-estate/buy/country-switzerland-fl'\n",
    "\n",
    "# Run the scraping function and display the results\n",
    "apartment_data = scrape_apartments(url)\n",
    "print(apartment_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n",
      "Scraping page 26...\n",
      "Scraping page 27...\n",
      "Scraping page 28...\n",
      "Scraping page 29...\n",
      "Scraping page 30...\n",
      "Scraping page 31...\n",
      "Scraping page 32...\n",
      "Scraping page 33...\n",
      "Scraping page 34...\n",
      "Scraping page 35...\n",
      "Scraping page 36...\n",
      "Scraping page 37...\n",
      "Scraping page 38...\n",
      "Scraping page 39...\n",
      "Scraping page 40...\n",
      "Scraping page 41...\n",
      "Scraping page 42...\n",
      "Scraping page 43...\n",
      "Scraping page 44...\n",
      "Scraping page 45...\n",
      "Scraping page 46...\n",
      "Scraping page 47...\n",
      "Scraping page 48...\n",
      "Scraping page 49...\n",
      "Scraping page 50...\n",
      "                                 Address  \\\n",
      "0                        1975 St-Séverin   \n",
      "1                  1470 Estavayer-le-Lac   \n",
      "2                            1510 Moudon   \n",
      "3    Allmendgütlistrasse 43, 8810 Horgen   \n",
      "4                           1964 Conthey   \n",
      "..                                   ...   \n",
      "995                     1213 Petit-Lancy   \n",
      "996            Grand Champsec, 1950 Sion   \n",
      "997            Rue du Jura, 1530 Payerne   \n",
      "998                6834 Morbio Inferiore   \n",
      "999                 1273 Arzier-Le Muids   \n",
      "\n",
      "                                                 Title  \\\n",
      "0    C'est sur les coteaux du soleil que votre futu...   \n",
      "1    Les Villas Lacustres - Villa 1 - Nouveaux plans !   \n",
      "2    Quartier Saint-Michel Appartement 2.5 pces au 1er   \n",
      "3               AURA - Letzte idyllische Gartenwohnung   \n",
      "4    Nouvelle promotion en cours de finition ! FRAI...   \n",
      "..                                                 ...   \n",
      "995          Villas contemporaines au bord de la Drize   \n",
      "996                          Lot J-101 - Studio au 1er   \n",
      "997  Immeuble de 3 appartements proche du centre ville   \n",
      "998                 FAVOLOSO APPARTAMENTO DI 3½ LOCALI   \n",
      "999  Rare et à saisir : VOTRE VILLA  INDIVIDUELLE N...   \n",
      "\n",
      "                                           Description      Rooms  \\\n",
      "0    Nous allons y construire 8 magnifiques villas ...  3.5 rooms   \n",
      "1    Situées dans un quartier résidentiel, à seulem...  3.5 rooms   \n",
      "2    LIVRAISON EN COURS? Appartement témoin à visit...  2.5 rooms   \n",
      "3    Termin online buchen - bitte Link kopieren: ht...  2.5 rooms   \n",
      "4    FRAIS D'ACHAT OFFERTS !Berra vous propose aujo...  3.5 rooms   \n",
      "..                                                 ...        ...   \n",
      "995                                                     4.5 rooms   \n",
      "996  Fitness, sauna, salle de jeux et de réunion, p...  2.5 rooms   \n",
      "997                                                                 \n",
      "998                                                     2.5 rooms   \n",
      "999  VOTRE VILLA NEUVE A MOINS DE 1.8 MILLIONS AUX ...  3.5 rooms   \n",
      "\n",
      "    Living Space (sqm)             Price  \n",
      "0                210m²   CHF 1,222,570.–  \n",
      "1                170m²   CHF 1,783,740.–  \n",
      "2                 60m²     CHF 699,170.–  \n",
      "3                130m²   CHF 2,044,140.–  \n",
      "4                170m²   CHF 1,022,070.–  \n",
      "..                 ...               ...  \n",
      "995              220m²   CHF 2,721,180.–  \n",
      "996               23m²     CHF 264,300.–  \n",
      "997                      CHF 3,255,000.–  \n",
      "998                     Price on request  \n",
      "999              180m²   CHF 2,285,010.–  \n",
      "\n",
      "[1000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_apartments(url):\n",
    "    # Setup ChromeDriver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run Chrome in headless mode (no GUI)\n",
    "    \n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    addresses = []\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    rooms = []\n",
    "    living_space = []\n",
    "    prices = []\n",
    "\n",
    "    # Loop through the pages (example range is 1 to 50)\n",
    "    for page_num in range(1, 51):\n",
    "        print(f\"Scraping page {page_num}...\")\n",
    "        page_url = f\"{url}?pn={page_num}\"\n",
    "        driver.get(page_url)\n",
    "        time.sleep(3)  # Allow time for the page to load fully\n",
    "\n",
    "        # Get the full content of the webpage\n",
    "        source = driver.page_source\n",
    "        soup = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "        # Extract addresses\n",
    "        address_elements = soup.find_all(class_='HgListingCard_address_JGiFv')\n",
    "        for element in address_elements:\n",
    "            address = element.find('address').text.strip()\n",
    "            addresses.append(address)\n",
    "\n",
    "        # Extract titles and descriptions\n",
    "        title_desc_elements = soup.find_all(class_='HgListingDescription_description_r5HCO')\n",
    "        for element in title_desc_elements:\n",
    "            title = element.find('span').text.strip()\n",
    "            description_elem = element.find('p', class_='HgListingDescription_large_uKs3J')\n",
    "            description = description_elem.text.strip() if description_elem else ''\n",
    "            titles.append(title)\n",
    "            descriptions.append(description)\n",
    "\n",
    "        # Extract rooms, living space, and prices\n",
    "        room_space_price_elements = soup.find_all(class_='HgListingRoomsLivingSpacePrice_roomsLivingSpacePrice_M6Ktp')\n",
    "        for element in room_space_price_elements:\n",
    "            strong_tags = element.find_all('strong')\n",
    "            rooms.append(strong_tags[0].text.strip() if strong_tags else '')\n",
    "            living_space.append(strong_tags[1].text.strip() if len(strong_tags) > 1 else '')\n",
    "            \n",
    "            # Extract price using the specific class name provided\n",
    "            price_elem = element.find('span', class_='HgListingRoomsLivingSpacePrice_price_u9Vee')\n",
    "            prices.append(price_elem.text.strip() if price_elem else 'Not available')\n",
    "\n",
    "    # Close the driver after scraping\n",
    "    driver.quit()\n",
    "\n",
    "    # Ensure all lists have the same length before creating DataFrame\n",
    "    min_length = min(len(addresses), len(titles), len(descriptions), len(rooms), len(living_space), len(prices))\n",
    "    df = pd.DataFrame({\n",
    "        'Address': addresses[:min_length],\n",
    "        'Title': titles[:min_length],\n",
    "        'Description': descriptions[:min_length],\n",
    "        'Rooms': rooms[:min_length],\n",
    "        'Living Space (sqm)': living_space[:min_length],\n",
    "        'Price': prices[:min_length]\n",
    "    })\n",
    "\n",
    "    # Save data to a CSV file\n",
    "    df.to_csv('immoscout24.csv', sep=\";\", index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for scraping\n",
    "url = 'https://www.immoscout24.ch/en/real-estate/buy/country-switzerland-fl'\n",
    "apartment_data = scrape_apartments(url)\n",
    "print(apartment_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping for each canton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping apartments in basel-landschaft...\n",
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n",
      "Scraping page 26...\n",
      "Scraping page 27...\n",
      "Scraping page 28...\n",
      "Scraping page 29...\n",
      "Scraping page 30...\n",
      "Scraping page 31...\n",
      "Scraping page 32...\n",
      "Scraping page 33...\n",
      "Scraping page 34...\n",
      "Scraping page 35...\n",
      "Scraping page 36...\n",
      "Scraping page 37...\n",
      "Scraping page 38...\n",
      "Scraping page 39...\n",
      "Scraping page 40...\n",
      "Scraping page 41...\n",
      "Scraping page 42...\n",
      "Scraping page 43...\n",
      "Scraping page 44...\n",
      "Scraping page 45...\n",
      "Scraping page 46...\n",
      "Scraping page 47...\n",
      "Scraping page 48...\n",
      "Scraping page 49...\n",
      "Scraping page 50...\n",
      "Scraping for basel-landschaft complete.\n",
      "Scraping apartments in st-gallen...\n",
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n",
      "Scraping page 26...\n",
      "Scraping page 27...\n",
      "Scraping page 28...\n",
      "Scraping page 29...\n",
      "Scraping page 30...\n",
      "Scraping page 31...\n",
      "Scraping page 32...\n",
      "Scraping page 33...\n",
      "Scraping page 34...\n",
      "Scraping page 35...\n",
      "Scraping page 36...\n",
      "Scraping page 37...\n",
      "Scraping page 38...\n",
      "Scraping page 39...\n",
      "Scraping page 40...\n",
      "Scraping page 41...\n",
      "Scraping page 42...\n",
      "Scraping page 43...\n",
      "Scraping page 44...\n",
      "Scraping page 45...\n",
      "Scraping page 46...\n",
      "Scraping page 47...\n",
      "Scraping page 48...\n",
      "Scraping page 49...\n",
      "Scraping page 50...\n",
      "Scraping for st-gallen complete.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_apartments(url):\n",
    "    # Setup ChromeDriver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run Chrome in headless mode (no GUI)\n",
    "    \n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    addresses = []\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    rooms = []\n",
    "    living_space = []\n",
    "    prices = []\n",
    "\n",
    "    # Loop through the pages (limited to first two pages)\n",
    "    for page_num in range(1, 51):\n",
    "        print(f\"Scraping page {page_num}...\")\n",
    "        page_url = f\"{url}?pn={page_num}\"\n",
    "        driver.get(page_url)\n",
    "        time.sleep(3)  # Allow time for the page to load fully\n",
    "\n",
    "        # Get the full content of the webpage\n",
    "        source = driver.page_source\n",
    "        soup = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "        # Extract addresses\n",
    "        address_elements = soup.find_all(class_='HgListingCard_address_JGiFv')\n",
    "        for element in address_elements:\n",
    "            address = element.find('address').text.strip()\n",
    "            addresses.append(address)\n",
    "\n",
    "        # Extract titles and descriptions\n",
    "        title_desc_elements = soup.find_all(class_='HgListingDescription_description_r5HCO')\n",
    "        for element in title_desc_elements:\n",
    "            title = element.find('span').text.strip()\n",
    "            description_elem = element.find('p', class_='HgListingDescription_large_uKs3J')\n",
    "            description = description_elem.text.strip() if description_elem else ''\n",
    "            titles.append(title)\n",
    "            descriptions.append(description)\n",
    "\n",
    "        # Extract rooms, living space, and prices\n",
    "        room_space_price_elements = soup.find_all(class_='HgListingRoomsLivingSpacePrice_roomsLivingSpacePrice_M6Ktp')\n",
    "        for element in room_space_price_elements:\n",
    "            strong_tags = element.find_all('strong')\n",
    "            rooms.append(strong_tags[0].text.strip() if strong_tags else '')\n",
    "            living_space.append(strong_tags[1].text.strip() if len(strong_tags) > 1 else '')\n",
    "            \n",
    "            # Extract price using the specific class name provided\n",
    "            price_elem = element.find('span', class_='HgListingRoomsLivingSpacePrice_price_u9Vee')\n",
    "            prices.append(price_elem.text.strip() if price_elem else 'Not available')\n",
    "\n",
    "    # Close the driver after scraping\n",
    "    driver.quit()\n",
    "\n",
    "    # Ensure all lists have the same length before creating DataFrame\n",
    "    min_length = min(len(addresses), len(titles), len(descriptions), len(rooms), len(living_space), len(prices))\n",
    "    df = pd.DataFrame({\n",
    "        'Address': addresses[:min_length],\n",
    "        'Title': titles[:min_length],\n",
    "        'Description': descriptions[:min_length],\n",
    "        'Rooms': rooms[:min_length],\n",
    "        'Living Space (sqm)': living_space[:min_length],\n",
    "        'Price': prices[:min_length]\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# List of cantons\n",
    "cantons = [\n",
    "    'Zurich', 'Bern', 'Lucerne', 'Uri', 'Schwyz', 'Obwalden', 'Nidwalden', 'Glarus', 'Zug', 'Fribourg', \n",
    "    'Solothurn', 'basel-landschaft', 'Basel-Stadt', 'Schaffhausen', 'appenzell-inner-rhoden', 'appenzell-ausser-rhoden', \n",
    "    'st-gallen', 'Graubuenden', 'Aargau', 'Thurgau', 'Ticino', 'Vaud', 'Valais', 'Neuchatel', 'Geneva', 'Jura'\n",
    "]\n",
    "\n",
    "# Scrape apartments for each canton\n",
    "for canton in cantons:\n",
    "    print(f\"Scraping apartments in {canton}...\")\n",
    "    url = f\"https://www.immoscout24.ch/en/real-estate/buy/canton-{canton.replace(' ', '-').lower()}\"\n",
    "    apartment_data = scrape_apartments(url)\n",
    "    # Save data to a CSV file for each canton\n",
    "    apartment_data.to_csv(f'immoscout24_{canton.replace(\" \", \"_\").lower()}.csv', sep=\";\", index=False)\n",
    "    print(f\"Scraping for {canton} complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory where CSV files are located\n",
    "directory = 'Data Cantons'\n",
    "\n",
    "# Get list of all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Read each CSV file and append to the list of DataFrames\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(directory, file), sep=';')\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('merged_immoscout24_data.csv', sep=';', index=False)\n",
    "\n",
    "print(\"Merged CSV file saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV file with canton names saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory where CSV files are located\n",
    "directory = 'Data Cantons'\n",
    "\n",
    "# Get list of all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Read each CSV file, add 'Canton' column and append to the list of DataFrames\n",
    "for file in csv_files:\n",
    "    canton_name = file.split('_')[1].split('.')[0].title()  # Extract canton name from file name\n",
    "    df = pd.read_csv(os.path.join(directory, file), sep=';')\n",
    "    df['Canton'] = canton_name\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('merged_immoscout24_data_with_canton.csv', sep=';', index=False)\n",
    "\n",
    "print(\"Merged CSV file with canton names saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
